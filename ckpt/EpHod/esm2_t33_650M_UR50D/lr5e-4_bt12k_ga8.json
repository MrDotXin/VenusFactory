{"hidden_size": 1280, "num_attention_head": 8, "attention_probs_dropout": 0.1, "plm_model": "facebook/esm2_t33_650M_UR50D", "pooling_method": "mean", "pooling_dropout": 0.1, "dataset": "AI4Protein/EpHod", "dataset_config": "data/EpHod/EpHod_HF.json", "normalize": null, "num_labels": 1, "problem_type": "regression", "sequence_column_name": "aa_seq", "label_column_name": "label", "pdb_type": null, "train_file": null, "valid_file": null, "test_file": null, "metrics": ["mse"], "seed": 3407, "learning_rate": 0.0005, "scheduler": null, "warmup_steps": 0, "num_workers": 4, "batch_size": null, "batch_token": 12000, "num_epochs": 100, "max_seq_len": -1, "gradient_accumulation_steps": 8, "max_grad_norm": -1, "patience": 10, "monitor": "mse", "monitor_strategy": "min", "training_method": "freeze", "lora_r": 8, "lora_alpha": 32, "lora_dropout": 0.1, "feedforward_modules": "w0", "lora_target_modules": ["query", "key", "value"], "structure_seq": [], "output_model_name": "lr5e-4_bt12k_ga8.pt", "output_root": "ckpt", "output_dir": "EpHod/esm2_t33_650M_UR50D", "wandb": false, "wandb_entity": null, "wandb_project": "VenusFactory", "wandb_run_name": null, "_column_override_info": {"sequence_column_name": {"default": "aa_seq"}, "label_column_name": {"default": "label"}}}